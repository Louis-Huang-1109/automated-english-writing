{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a700e",
   "metadata": {
    "id": "329a700e",
    "outputId": "37077604-9466-408e-dab8-37965759ca15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-posthocsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading scikit_posthocs-0.9.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-posthocs) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-posthocs) (1.11.4)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-posthocs) (0.14.0)\n",
      "Requirement already satisfied: pandas>=0.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-posthocs) (2.1.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-posthocs) (0.12.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-posthocs) (3.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=0.20.0->scikit-posthocs) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->scikit-posthocs) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->scikit-posthocs) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->scikit-posthocs) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->scikit-posthocs) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->scikit-posthocs) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->scikit-posthocs) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->scikit-posthocs) (3.0.9)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from statsmodels->scikit-posthocs) (0.5.3)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels->scikit-posthocs) (1.16.0)\n",
      "Downloading scikit_posthocs-0.9.0-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: scikit-posthocs\n",
      "Successfully installed scikit-posthocs-0.9.0\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33989044",
   "metadata": {
    "id": "33989044"
   },
   "outputs": [],
   "source": [
    "import scikit_posthocs as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Create samples\n",
    "df = pd.read_excel(\"Lexical Complexity.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "644c635f",
   "metadata": {
    "id": "644c635f"
   },
   "outputs": [],
   "source": [
    "# lexical density\n",
    "\n",
    "a1_lexical_density = []\n",
    "a2_lexical_density = []\n",
    "b1_lexical_density = []\n",
    "b2_lexical_density = []\n",
    "c1_lexical_density = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    density = row[\"Lexical Density\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_lexical_density.append(density)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_lexical_density.append(density)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_lexical_density.append(density)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_lexical_density.append(density)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_lexical_density.append(density)\n",
    "\n",
    "#lexical sophistication_I\n",
    "a1_lexical_sophistication_I = []\n",
    "a2_lexical_sophistication_I = []\n",
    "b1_lexical_sophistication_I = []\n",
    "b2_lexical_sophistication_I = []\n",
    "c1_lexical_sophistication_I = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    sophistication = row[\"Lexical Sophistication-I\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_lexical_sophistication_I.append(sophistication)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_lexical_sophistication_I.append(sophistication)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_lexical_sophistication_I.append(sophistication)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_lexical_sophistication_I.append(sophistication)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_lexical_sophistication_I.append(sophistication)\n",
    "\n",
    "#lexical sophistication_II\n",
    "a1_lexical_sophistication_II = []\n",
    "a2_lexical_sophistication_II = []\n",
    "b1_lexical_sophistication_II = []\n",
    "b2_lexical_sophistication_II = []\n",
    "c1_lexical_sophistication_II = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    sophistication = row[\"Lexical Sophistication-II\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_lexical_sophistication_II.append(sophistication)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_lexical_sophistication_II.append(sophistication)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_lexical_sophistication_II.append(sophistication)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_lexical_sophistication_II.append(sophistication)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_lexical_sophistication_II.append(sophistication)\n",
    "\n",
    "\n",
    "# verb_sophistication_I\n",
    "a1_verb_sophistication_I = []\n",
    "a2_verb_sophistication_I = []\n",
    "b1_verb_sophistication_I = []\n",
    "b2_verb_sophistication_I = []\n",
    "c1_verb_sophistication_I = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    sophistication = row[\"Verb Sophistication-II\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_verb_sophistication_I.append(sophistication)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_verb_sophistication_I.append(sophistication)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_verb_sophistication_I.append(sophistication)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_verb_sophistication_I.append(sophistication)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_verb_sophistication_I.append(sophistication)\n",
    "\n",
    "\n",
    "# corrected_vs1\n",
    "a1_corrected_vs1 = []\n",
    "a2_corrected_vs1 = []\n",
    "b1_corrected_vs1 = []\n",
    "b2_corrected_vs1 = []\n",
    "c1_corrected_vs1 = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    ratio = row[\"Corrected VS1\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_corrected_vs1.append(ratio)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_corrected_vs1.append(ratio)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_corrected_vs1.append(ratio)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_corrected_vs1.append(ratio)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_corrected_vs1.append(ratio)\n",
    "\n",
    "# verb_sophistication_II\n",
    "a1_verb_sophistication_II = []\n",
    "a2_verb_sophistication_II = []\n",
    "b1_verb_sophistication_II = []\n",
    "b2_verb_sophistication_II = []\n",
    "c1_verb_sophistication_II = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    sophistication = row[\"Verb Sophistication-II\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_verb_sophistication_II.append(sophistication)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_verb_sophistication_II.append(sophistication)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_verb_sophistication_II.append(sophistication)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_verb_sophistication_II.append(sophistication)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_verb_sophistication_II.append(sophistication)\n",
    "\n",
    "# type token ratio\n",
    "a1_ttr = []\n",
    "a2_ttr = []\n",
    "b1_ttr = []\n",
    "b2_ttr = []\n",
    "c1_ttr = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    ttr = row[\"Typeâ€“Token Ratio\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_ttr.append(ttr)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_ttr.append(ttr)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_ttr.append(ttr)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_ttr.append(ttr)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_ttr.append(ttr)\n",
    "\n",
    "# number_of_different_words\n",
    "a1_number_of_different_words = []\n",
    "a2_number_of_different_words = []\n",
    "b1_number_of_different_words = []\n",
    "b2_number_of_different_words = []\n",
    "c1_number_of_different_words = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    num_words = row[\"Number of Different Words\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_number_of_different_words.append(num_words)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_number_of_different_words.append(num_words)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_number_of_different_words.append(num_words)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_number_of_different_words.append(num_words)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_number_of_different_words.append(num_words)\n",
    "\n",
    "# corrected_ttr\n",
    "a1_corrected_ttr = []\n",
    "a2_corrected_ttr = []\n",
    "b1_corrected_ttr = []\n",
    "b2_corrected_ttr = []\n",
    "c1_corrected_ttr = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    ttr = row[\"Corrected TTR\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_corrected_ttr.append(ttr)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_corrected_ttr.append(ttr)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_corrected_ttr.append(ttr)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_corrected_ttr.append(ttr)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_corrected_ttr.append(ttr)\n",
    "\n",
    "# root_ttr\n",
    "a1_root_ttr = []\n",
    "a2_root_ttr = []\n",
    "b1_root_ttr = []\n",
    "b2_root_ttr = []\n",
    "c1_root_ttr = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    root_ttr = row[\"Root TTR\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_root_ttr.append(root_ttr)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_root_ttr.append(root_ttr)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_root_ttr.append(root_ttr)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_root_ttr.append(root_ttr)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_root_ttr.append(root_ttr)\n",
    "\n",
    "#bilogarithmic_ttr\n",
    "a1_bilogarithmic_ttr = []\n",
    "a2_bilogarithmic_ttr = []\n",
    "b1_bilogarithmic_ttr = []\n",
    "b2_bilogarithmic_ttr = []\n",
    "c1_bilogarithmic_ttr = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    root_ttr = row[\"Bilogarithmic TTR\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_bilogarithmic_ttr.append(root_ttr)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_bilogarithmic_ttr.append(root_ttr)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_bilogarithmic_ttr.append(root_ttr)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_bilogarithmic_ttr.append(root_ttr)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_bilogarithmic_ttr.append(root_ttr)\n",
    "\n",
    "# uber_index\n",
    "a1_uber_index = []\n",
    "a2_uber_index = []\n",
    "b1_uber_index = []\n",
    "b2_uber_index = []\n",
    "c1_uber_index = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    uber_index = row[\"Uber Index\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_uber_index.append(uber_index)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_uber_index.append(uber_index)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_uber_index.append(uber_index)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_uber_index.append(uber_index)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_uber_index.append(uber_index)\n",
    "\n",
    "# lexical_word_variation\n",
    "a1_lexical_word_variation = []\n",
    "a2_lexical_word_variation = []\n",
    "b1_lexical_word_variation = []\n",
    "b2_lexical_word_variation = []\n",
    "c1_lexical_word_variation = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    word_variation = row[\"Lexical Word Variation\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_lexical_word_variation.append(word_variation)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_lexical_word_variation.append(word_variation)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_lexical_word_variation.append(word_variation)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_lexical_word_variation.append(word_variation)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_lexical_word_variation.append(word_variation)\n",
    "\n",
    "# verb_variation_I\n",
    "a1_verb_variation_I = []\n",
    "a2_verb_variation_I = []\n",
    "b1_verb_variation_I = []\n",
    "b2_verb_variation_I = []\n",
    "c1_verb_variation_I = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    verb_variation = row[\"Verb Variation-I\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_verb_variation_I.append(verb_variation)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_verb_variation_I.append(verb_variation)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_verb_variation_I.append(verb_variation)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_verb_variation_I.append(verb_variation)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_verb_variation_I.append(verb_variation)\n",
    "\n",
    "# squared_vv1\n",
    "a1_squared_vv1 = []\n",
    "a2_squared_vv1 = []\n",
    "b1_squared_vv1 = []\n",
    "b2_squared_vv1 = []\n",
    "c1_squared_vv1 = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    squared_vv1 = row[\"Squared VV1\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_squared_vv1.append(squared_vv1)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_squared_vv1.append(squared_vv1)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_squared_vv1.append(squared_vv1)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_squared_vv1.append(squared_vv1)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_squared_vv1.append(squared_vv1)\n",
    "\n",
    "# corrected_vv1\n",
    "a1_corrected_vv1 = []\n",
    "a2_corrected_vv1 = []\n",
    "b1_corrected_vv1 = []\n",
    "b2_corrected_vv1 = []\n",
    "c1_corrected_vv1 = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    corrected_vv1 = row[\"Corrected VV1\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_corrected_vv1.append(corrected_vv1)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_corrected_vv1.append(corrected_vv1)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_corrected_vv1.append(corrected_vv1)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_corrected_vv1.append(corrected_vv1)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_corrected_vv1.append(corrected_vv1)\n",
    "\n",
    "\n",
    "# verb_variation_II\n",
    "a1_verb_variation_II = []\n",
    "a2_verb_variation_II = []\n",
    "b1_verb_variation_II = []\n",
    "b2_verb_variation_II = []\n",
    "c1_verb_variation_II = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    verb_variation = row[\"Verb Variation-II\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_verb_variation_II.append(verb_variation)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_verb_variation_II.append(verb_variation)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_verb_variation_II.append(verb_variation)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_verb_variation_II.append(verb_variation)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_verb_variation_II.append(verb_variation)\n",
    "\n",
    "# noun_variation\n",
    "a1_noun_variation = []\n",
    "a2_noun_variation = []\n",
    "b1_noun_variation = []\n",
    "b2_noun_variation = []\n",
    "c1_noun_variation = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    noun_variation = row[\"Noun Variation\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_noun_variation.append(noun_variation)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_noun_variation.append(noun_variation)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_noun_variation.append(noun_variation)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_noun_variation.append(noun_variation)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_noun_variation.append(noun_variation)\n",
    "\n",
    "# adjective_variation\n",
    "a1_adjective_variation = []\n",
    "a2_adjective_variation = []\n",
    "b1_adjective_variation = []\n",
    "b2_adjective_variation = []\n",
    "c1_adjective_variation = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    adjective_variation = row[\"Adjective Variation\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_adjective_variation.append(adjective_variation)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_adjective_variation.append(adjective_variation)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_adjective_variation.append(adjective_variation)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_adjective_variation.append(adjective_variation)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_adjective_variation.append(adjective_variation)\n",
    "\n",
    "# adverb_variation\n",
    "a1_adverb_variation = []\n",
    "a2_adverb_variation = []\n",
    "b1_adverb_variation = []\n",
    "b2_adverb_variation = []\n",
    "c1_adverb_variation = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    adverb_variation = row[\"Adverb Variation\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_adverb_variation.append(adverb_variation)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_adverb_variation.append(adverb_variation)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_adverb_variation.append(adverb_variation)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_adverb_variation.append(adverb_variation)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_adverb_variation.append(adverb_variation)\n",
    "\n",
    "# modifier_variation\n",
    "a1_modifier_variation = []\n",
    "a2_modifier_variation = []\n",
    "b1_modifier_variation = []\n",
    "b2_modifier_variation = []\n",
    "c1_modifier_variation = []\n",
    "for index, row in df.iterrows():\n",
    "    cefr = row[\"CEFR\"]\n",
    "    modifier_variation = row[\"Modifier Variation\"]\n",
    "    if cefr == \"A1\":\n",
    "        a1_modifier_variation.append(modifier_variation)\n",
    "    elif cefr == \"A2\":\n",
    "        a2_modifier_variation.append(modifier_variation)\n",
    "    elif cefr == \"B1\":\n",
    "        b1_modifier_variation.append(modifier_variation)\n",
    "    elif cefr == \"B2\":\n",
    "        b2_modifier_variation.append(modifier_variation)\n",
    "    elif cefr == \"C1\":\n",
    "        c1_modifier_variation.append(modifier_variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd3c01ea",
   "metadata": {
    "id": "cd3c01ea"
   },
   "outputs": [],
   "source": [
    "max_length = max(len(a1_lexical_density),len(a2_lexical_density),len(b1_lexical_density) ,len(b2_lexical_density),len(c1_lexical_density))\n",
    "\n",
    "\n",
    "\n",
    "#x = pd.DataFrame({\"TTR_A1\": a1_ttr, \"TTR_A2\":a2_ttr ,\n",
    "#                 \"TTR_B1\": b1_ttr,\"TTR_B2\":b2_ttr,\"TTR_C1\":c1_ttr})\n",
    "#x = x.melt(var_name='groups', value_name='values')\n",
    "#sp.posthoc_scheffe(x, val_col='values', group_col='groups')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f0efba26",
   "metadata": {
    "id": "f0efba26"
   },
   "outputs": [],
   "source": [
    "def same_size(a1,a2,b1,b2,c1):\n",
    "    a1 += [np.nan] * (max_length - len(a1))\n",
    "    a2 += [np.nan] * (max_length - len(a2))\n",
    "    b1 += [np.nan] * (max_length - len(b1))\n",
    "    b2 += [np.nan] * (max_length - len(b2))\n",
    "    c1 += [np.nan] * (max_length - len(c1))\n",
    "\n",
    "same_size(a1_lexical_density, a2_lexical_density, b1_lexical_density, b2_lexical_density, c1_lexical_density)\n",
    "same_size(a1_lexical_sophistication_I, a2_lexical_sophistication_I, b1_lexical_sophistication_I, b2_lexical_sophistication_I, c1_lexical_sophistication_I)\n",
    "same_size(a1_lexical_sophistication_II, a2_lexical_sophistication_II, b1_lexical_sophistication_II, b2_lexical_sophistication_II, c1_lexical_sophistication_II)\n",
    "same_size(a1_verb_sophistication_I, a2_verb_sophistication_I, b1_verb_sophistication_I, b2_verb_sophistication_I, c1_verb_sophistication_I)\n",
    "same_size(a1_corrected_vs1, a2_corrected_vs1, b1_corrected_vs1, b2_corrected_vs1, c1_corrected_vs1)\n",
    "same_size(a1_verb_sophistication_II, a2_verb_sophistication_II, b1_verb_sophistication_II, b2_verb_sophistication_II, c1_verb_sophistication_II)\n",
    "same_size(a1_ttr, a2_ttr, b1_ttr, b2_ttr, c1_ttr)\n",
    "same_size(a1_number_of_different_words, a2_number_of_different_words, b1_number_of_different_words, b2_number_of_different_words, c1_number_of_different_words)\n",
    "same_size(a1_corrected_ttr, a2_corrected_ttr, b1_corrected_ttr, b2_corrected_ttr, c1_corrected_ttr)\n",
    "same_size(a1_root_ttr, a2_root_ttr, b1_root_ttr, b2_root_ttr, c1_root_ttr)\n",
    "same_size(a1_bilogarithmic_ttr, a2_bilogarithmic_ttr, b1_bilogarithmic_ttr, b2_bilogarithmic_ttr, c1_bilogarithmic_ttr)\n",
    "same_size(a1_uber_index, a2_uber_index, b1_uber_index, b2_uber_index, c1_uber_index)\n",
    "same_size(a1_lexical_word_variation, a2_lexical_word_variation, b1_lexical_word_variation, b2_lexical_word_variation, c1_lexical_word_variation)\n",
    "same_size(a1_verb_variation_I, a2_verb_variation_I, b1_verb_variation_I, b2_verb_variation_I, c1_verb_variation_I)\n",
    "same_size(a1_squared_vv1, a2_squared_vv1, b1_squared_vv1, b2_squared_vv1, c1_squared_vv1)\n",
    "same_size(a1_corrected_vv1, a2_corrected_vv1, b1_corrected_vv1, b2_corrected_vv1, c1_corrected_vv1)\n",
    "same_size(a1_verb_variation_II, a2_verb_variation_II, b1_verb_variation_II, b2_verb_variation_II, c1_verb_variation_II)\n",
    "same_size(a1_noun_variation, a2_noun_variation, b1_noun_variation, b2_noun_variation, c1_noun_variation)\n",
    "same_size(a1_adjective_variation, a2_adjective_variation, b1_adjective_variation, b2_adjective_variation, c1_adjective_variation)\n",
    "same_size(a1_adverb_variation, a2_adverb_variation, b1_adverb_variation, b2_adverb_variation, c1_adverb_variation)\n",
    "same_size(a1_modifier_variation, a2_modifier_variation, b1_modifier_variation, b2_modifier_variation, c1_modifier_variation)\n",
    "\n",
    "\n",
    "\n",
    "#a1_lexical_sophistication_I += [np.nan] * (max_length - len(a1_lexical_sophistication_I))\n",
    "#a2_lexical_sophistication_I += [np.nan] * (max_length - len(a2_lexical_sophistication_I))\n",
    "#b1_lexical_sophistication_I += [np.nan] * (max_length - len(b1_lexical_sophistication_I))\n",
    "#b2_lexical_sophistication_I += [np.nan] * (max_length - len(b2_lexical_sophistication_I))\n",
    "#c1_lexical_sophistication_I += [np.nan] * (max_length - len(c1_lexical_sophistication_I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b00fbb94",
   "metadata": {
    "id": "b00fbb94",
    "outputId": "6d0374fb-a963-4ab9-c0f8-0c7e4ebb3f85"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CEFR</th>\n",
       "      <th>Lexical_Density</th>\n",
       "      <th>Lexical_Sophistication_I</th>\n",
       "      <th>Lexical_Sophistication_II</th>\n",
       "      <th>Verb_Sophistication_I</th>\n",
       "      <th>Corrected_VS1</th>\n",
       "      <th>Verb_Sophistication_II</th>\n",
       "      <th>Number_of_Different_Words</th>\n",
       "      <th>Type_Token_Ratio</th>\n",
       "      <th>Corrected_TTR</th>\n",
       "      <th>...</th>\n",
       "      <th>Uber_Index</th>\n",
       "      <th>Lexical_Word_Variation</th>\n",
       "      <th>Verb_Variation_I</th>\n",
       "      <th>Squared_VV1</th>\n",
       "      <th>Corrected_VV1</th>\n",
       "      <th>Verb_Variation_II</th>\n",
       "      <th>Noun_Variation</th>\n",
       "      <th>Adjective_Variation</th>\n",
       "      <th>Adverb_Variation</th>\n",
       "      <th>Modifier_Variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.351351</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>37</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>3.900142</td>\n",
       "      <td>...</td>\n",
       "      <td>28.056221</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.870829</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1</td>\n",
       "      <td>0.146341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>2.711088</td>\n",
       "      <td>...</td>\n",
       "      <td>13.757318</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>2.969287</td>\n",
       "      <td>...</td>\n",
       "      <td>18.467565</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.224745</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.518519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>3.097112</td>\n",
       "      <td>...</td>\n",
       "      <td>15.356074</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>4.082483</td>\n",
       "      <td>...</td>\n",
       "      <td>30.632486</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406057</th>\n",
       "      <td>C1</td>\n",
       "      <td>0.434524</td>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.894427</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>98</td>\n",
       "      <td>0.632258</td>\n",
       "      <td>5.566026</td>\n",
       "      <td>...</td>\n",
       "      <td>15.870876</td>\n",
       "      <td>0.780822</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>8.450000</td>\n",
       "      <td>2.055480</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.301370</td>\n",
       "      <td>0.164384</td>\n",
       "      <td>0.164384</td>\n",
       "      <td>0.328767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406058</th>\n",
       "      <td>C1</td>\n",
       "      <td>0.416268</td>\n",
       "      <td>0.367816</td>\n",
       "      <td>0.316239</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>1.961161</td>\n",
       "      <td>3.846154</td>\n",
       "      <td>117</td>\n",
       "      <td>0.657303</td>\n",
       "      <td>6.200988</td>\n",
       "      <td>...</td>\n",
       "      <td>17.815925</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>18.615385</td>\n",
       "      <td>3.050851</td>\n",
       "      <td>0.252874</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.080460</td>\n",
       "      <td>0.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406059</th>\n",
       "      <td>C1</td>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>2.228344</td>\n",
       "      <td>4.965517</td>\n",
       "      <td>116</td>\n",
       "      <td>0.640884</td>\n",
       "      <td>6.096825</td>\n",
       "      <td>...</td>\n",
       "      <td>16.857115</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>23.310345</td>\n",
       "      <td>3.413967</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.130952</td>\n",
       "      <td>0.226190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406060</th>\n",
       "      <td>C1</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.355263</td>\n",
       "      <td>0.311828</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.236068</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>93</td>\n",
       "      <td>0.553571</td>\n",
       "      <td>5.073566</td>\n",
       "      <td>...</td>\n",
       "      <td>12.500442</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>3.004164</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.078947</td>\n",
       "      <td>0.236842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406061</th>\n",
       "      <td>C1</td>\n",
       "      <td>0.365385</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>4.840000</td>\n",
       "      <td>92</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>4.707129</td>\n",
       "      <td>...</td>\n",
       "      <td>10.373150</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>19.360000</td>\n",
       "      <td>3.111270</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.065789</td>\n",
       "      <td>0.171053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>406062 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CEFR  Lexical_Density  Lexical_Sophistication_I  \\\n",
       "0        A1         0.296296                  0.312500   \n",
       "1        A1         0.146341                  0.000000   \n",
       "2        A1         0.222222                  0.125000   \n",
       "3        A1         0.234043                  0.363636   \n",
       "4        A1         0.293103                  0.176471   \n",
       "...     ...              ...                       ...   \n",
       "406057   C1         0.434524                  0.301370   \n",
       "406058   C1         0.416268                  0.367816   \n",
       "406059   C1         0.407767                  0.369048   \n",
       "406060   C1         0.404255                  0.355263   \n",
       "406061   C1         0.365385                  0.407895   \n",
       "\n",
       "        Lexical_Sophistication_II  Verb_Sophistication_I  Corrected_VS1  \\\n",
       "0                        0.351351               0.285714       0.755929   \n",
       "1                        0.380952               0.000000       0.000000   \n",
       "2                        0.478261               0.000000       0.000000   \n",
       "3                        0.518519               0.000000       0.000000   \n",
       "4                        0.325000               0.000000       0.000000   \n",
       "...                           ...                    ...            ...   \n",
       "406057                   0.346939               0.200000       0.894427   \n",
       "406058                   0.316239               0.384615       1.961161   \n",
       "406059                   0.293103               0.413793       2.228344   \n",
       "406060                   0.311828               0.500000       2.236068   \n",
       "406061                   0.326087               0.440000       2.200000   \n",
       "\n",
       "        Verb_Sophistication_II  Number_of_Different_Words  Type_Token_Ratio  \\\n",
       "0                     0.571429                         37          0.822222   \n",
       "1                     0.000000                         21          0.700000   \n",
       "2                     0.000000                         23          0.766667   \n",
       "3                     0.000000                         27          0.710526   \n",
       "4                     0.000000                         40          0.833333   \n",
       "...                        ...                        ...               ...   \n",
       "406057                0.800000                         98          0.632258   \n",
       "406058                3.846154                        117          0.657303   \n",
       "406059                4.965517                        116          0.640884   \n",
       "406060                5.000000                         93          0.553571   \n",
       "406061                4.840000                         92          0.481675   \n",
       "\n",
       "        Corrected_TTR  ...  Uber_Index  Lexical_Word_Variation  \\\n",
       "0            3.900142  ...   28.056221                1.000000   \n",
       "1            2.711088  ...   13.757318                0.666667   \n",
       "2            2.969287  ...   18.467565                1.000000   \n",
       "3            3.097112  ...   15.356074                0.818182   \n",
       "4            4.082483  ...   30.632486                0.941176   \n",
       "...               ...  ...         ...                     ...   \n",
       "406057       5.566026  ...   15.870876                0.780822   \n",
       "406058       6.200988  ...   17.815925                0.827586   \n",
       "406059       6.096825  ...   16.857115                0.869048   \n",
       "406060       5.073566  ...   12.500442                0.815789   \n",
       "406061       4.707129  ...   10.373150                0.736842   \n",
       "\n",
       "        Verb_Variation_I  Squared_VV1  Corrected_VV1  Verb_Variation_II  \\\n",
       "0               1.000000     7.000000       1.870829           0.437500   \n",
       "1               0.000000     0.000000       0.000000           0.000000   \n",
       "2               1.000000     3.000000       1.224745           0.375000   \n",
       "3               1.000000     1.000000       0.707107           0.090909   \n",
       "4               0.800000     3.200000       1.264911           0.235294   \n",
       "...                  ...          ...            ...                ...   \n",
       "406057          0.650000     8.450000       2.055480           0.178082   \n",
       "406058          0.846154    18.615385       3.050851           0.252874   \n",
       "406059          0.896552    23.310345       3.413967           0.309524   \n",
       "406060          0.950000    18.050000       3.004164           0.250000   \n",
       "406061          0.880000    19.360000       3.111270           0.289474   \n",
       "\n",
       "        Noun_Variation  Adjective_Variation  Adverb_Variation  \\\n",
       "0             0.312500             0.250000          0.000000   \n",
       "1             0.500000             0.166667          0.000000   \n",
       "2             0.250000             0.375000          0.000000   \n",
       "3             0.454545             0.272727          0.000000   \n",
       "4             0.529412             0.176471          0.000000   \n",
       "...                ...                  ...               ...   \n",
       "406057        0.301370             0.164384          0.164384   \n",
       "406058        0.310345             0.195402          0.080460   \n",
       "406059        0.333333             0.095238          0.130952   \n",
       "406060        0.328947             0.157895          0.078947   \n",
       "406061        0.302632             0.105263          0.065789   \n",
       "\n",
       "        Modifier_Variation  \n",
       "0                 0.250000  \n",
       "1                 0.166667  \n",
       "2                 0.375000  \n",
       "3                 0.272727  \n",
       "4                 0.176471  \n",
       "...                    ...  \n",
       "406057            0.328767  \n",
       "406058            0.275862  \n",
       "406059            0.226190  \n",
       "406060            0.236842  \n",
       "406061            0.171053  \n",
       "\n",
       "[406062 rows x 22 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc_df = pd.read_excel(\"Lexical Complexity.xlsx\")\n",
    "\n",
    "new_cols = {}\n",
    "for c in lc_df.columns:\n",
    "    new_cols[c] = c.replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "\n",
    "lc_df = lc_df.rename(columns=new_cols)\n",
    "lc_df.rename(columns={'Typeâ€“Token_Ratio':'Type_Token_Ratio'}, inplace=True)\n",
    "lc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2a367066",
   "metadata": {
    "id": "2a367066",
    "outputId": "a003974d-ff89-4e1f-e4a1-9b89799c7234"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group1</th>\n",
       "      <th>group2</th>\n",
       "      <th>stat</th>\n",
       "      <th>pval</th>\n",
       "      <th>pval_corr</th>\n",
       "      <th>reject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>A2</td>\n",
       "      <td>-259.6304</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1</td>\n",
       "      <td>B1</td>\n",
       "      <td>-231.7080</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1</td>\n",
       "      <td>B2</td>\n",
       "      <td>-132.0329</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1</td>\n",
       "      <td>C1</td>\n",
       "      <td>-86.2926</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2</td>\n",
       "      <td>B1</td>\n",
       "      <td>-47.3889</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A2</td>\n",
       "      <td>B2</td>\n",
       "      <td>-30.2947</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A2</td>\n",
       "      <td>C1</td>\n",
       "      <td>-38.7514</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B1</td>\n",
       "      <td>B2</td>\n",
       "      <td>-1.9982</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>0.457</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B1</td>\n",
       "      <td>C1</td>\n",
       "      <td>-27.0335</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B2</td>\n",
       "      <td>C1</td>\n",
       "      <td>-24.8874</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  group1 group2      stat    pval  pval_corr  reject\n",
       "0     A1     A2 -259.6304  0.0000      0.000    True\n",
       "1     A1     B1 -231.7080  0.0000      0.000    True\n",
       "2     A1     B2 -132.0329  0.0000      0.000    True\n",
       "3     A1     C1  -86.2926  0.0000      0.000    True\n",
       "4     A2     B1  -47.3889  0.0000      0.000    True\n",
       "5     A2     B2  -30.2947  0.0000      0.000    True\n",
       "6     A2     C1  -38.7514  0.0000      0.000    True\n",
       "7     B1     B2   -1.9982  0.0457      0.457   False\n",
       "8     B1     C1  -27.0335  0.0000      0.000    True\n",
       "9     B2     C1  -24.8874  0.0000      0.000    True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nheatmap_data = post_hoc_df.pivot(\"group1\", \"group2\", \"reject\").astype(float)\\n\\n# Convert boolean True/False to numeric values for plotting\\nheatmap_data = heatmap_data.replace({True: 1, False: 0})\\n\\n# Create a heatmap\\nplt.figure(figsize=(10, 8))\\nsns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\", cbar_kws={\\'label\\': \\'Significant Difference (0=Accept, 1=Reject)\\'},\\n            vmin=0, vmax=1)\\nplt.title(\\'Heatmap of Significant Differences of \\'+lc_index+\\' Between Groups\\')\\nplt.show()'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.stats.multicomp as mc\n",
    "from scipy.stats import ttest_ind\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scikit_posthocs as sp\n",
    "import pandas as pd\n",
    "\n",
    "ld_index = \"Lexical_Density\"\n",
    "ls1_index = \"Lexical_Sophistication_I\"\n",
    "\"\"\"ls2_index = \"Lexical_Sophistication_II\"\n",
    "vs1 = \"Verb_Sophistication_I\"\n",
    "cvs1 = \"Corrected_VS1\"\n",
    "vs2 = \"Verb_Sophistication_II\"\n",
    "ndw = \"Number_of_Different Words\"\n",
    "ttr = \"Type_Token_Ratio\"\n",
    "cttr = \"Corrected_TTR\"\n",
    "rttr = \"Root_TTR\"\n",
    "bittr = \"Bilogarithmic_TTR\"\n",
    "uber = \"Uber_Index\"\n",
    "lwv = \"Lexical_Word_Variation\"\n",
    "vv1=\"Verb_Variation_I\"\n",
    "svv1 = \"Squared_VV1\"\n",
    "cvv1 = \"Corrected_VV1\"\n",
    "vv2 = \"Verb_Variation_II\"\n",
    "nv = \"Noun_Variation\"\n",
    "adjv = \"Adjective_Variation\"\n",
    "adv = \"Adverb_Variation\"\n",
    "mv = \"Modifier_Variation\"\n",
    "maas = \"Maas_TTR\"\n",
    "hdd = \"HDD\"\n",
    "mtld = \"MTLD\"\"\"\n",
    "\n",
    "## post-hoc\n",
    "def post_hoc_bonferroni(index):\n",
    "    comp = mc.MultiComparison(lc_df[index], lc_df['CEFR'])\n",
    "    post_hoc_res = comp.allpairtest(ttest_ind, method='bonf')\n",
    "    post_hoc_df = pd.DataFrame(post_hoc_res[2])\n",
    "    return post_hoc_df\n",
    "def post_hoc_scheffe(a1,a2,b1,b2,c1):\n",
    "    x = pd.DataFrame({\"A1\":a1 , \"A2\": a2, \"B1\":b2,\"B2\":b2,\"C1\":c1})\n",
    "    x = x.melt(var_name='groups', value_name='values')\n",
    "    post_hoc = sp.posthoc_scheffe(x, val_col='values', group_col='groups')\n",
    "    return post_hoc\n",
    "\n",
    "# ## tukeyhsd\n",
    "# post_hoc_res = comp.tukeyhsd()\n",
    "\n",
    "# post_hoc_df = pd.DataFrame.from_records(post_hoc_res.summary().data)\n",
    "# header = post_hoc_df.iloc[0] # grab the first row for the header\n",
    "# post_hoc_df = post_hoc_df[1:] # take the data less the header row\n",
    "# post_hoc_df.columns = header\n",
    "\n",
    "# display(post_hoc_df)\n",
    "\n",
    "#comp = mc.MultiComparison(lc_df[index], lc_df['CEFR'])\n",
    "\n",
    "post_hoc_df = post_hoc_bonferroni(ld_index)\n",
    "## bonferroni\n",
    "#post_hoc_res = comp.allpairtest(ttest_ind, method='bonf')\n",
    "#post_hoc_df = pd.DataFrame(post_hoc_res[2])\n",
    "\n",
    "display(post_hoc_df)\n",
    "\n",
    "\"\"\"\n",
    "heatmap_data = post_hoc_df.pivot(\"group1\", \"group2\", \"reject\").astype(float)\n",
    "\n",
    "# Convert boolean True/False to numeric values for plotting\n",
    "heatmap_data = heatmap_data.replace({True: 1, False: 0})\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap=\"coolwarm\", cbar_kws={'label': 'Significant Difference (0=Accept, 1=Reject)'},\n",
    "            vmin=0, vmax=1)\n",
    "plt.title('Heatmap of Significant Differences of '+lc_index+' Between Groups')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7ede75da",
   "metadata": {
    "id": "7ede75da"
   },
   "outputs": [],
   "source": [
    "#Result for all indices\n",
    "ld = post_hoc_scheffe(a1_lexical_density, a2_lexical_density, b1_lexical_density, b2_lexical_density, c1_lexical_density).round(2)\n",
    "ls1 = post_hoc_scheffe(a1_lexical_sophistication_I, a2_lexical_sophistication_I, b1_lexical_sophistication_I, b2_lexical_sophistication_I, c1_lexical_sophistication_I).round(2)\n",
    "ls2 = post_hoc_scheffe(a1_lexical_sophistication_II, a2_lexical_sophistication_II, b1_lexical_sophistication_II, b2_lexical_sophistication_II, c1_lexical_sophistication_II).round(2)\n",
    "vs1 = post_hoc_scheffe(a1_verb_sophistication_I, a2_verb_sophistication_I, b1_verb_sophistication_I, b2_verb_sophistication_I, c1_verb_sophistication_I).round(2)\n",
    "cvs1 = post_hoc_scheffe(a1_corrected_vs1, a2_corrected_vs1, b1_corrected_vs1, b2_corrected_vs1, c1_corrected_vs1).round(2)\n",
    "vs2 = post_hoc_scheffe(a1_verb_sophistication_II, a2_verb_sophistication_II, b1_verb_sophistication_II, b2_verb_sophistication_II, c1_verb_sophistication_II).round(2)\n",
    "ttr = post_hoc_scheffe(a1_ttr, a2_ttr, b1_ttr, b2_ttr, c1_ttr).round(2)\n",
    "ndw = post_hoc_scheffe(a1_number_of_different_words, a2_number_of_different_words, b1_number_of_different_words, b2_number_of_different_words, c1_number_of_different_words).round(2)\n",
    "cttr = post_hoc_scheffe(a1_corrected_ttr, a2_corrected_ttr, b1_corrected_ttr, b2_corrected_ttr, c1_corrected_ttr).round(2)\n",
    "rttr = post_hoc_scheffe(a1_root_ttr, a2_root_ttr, b1_root_ttr, b2_root_ttr, c1_root_ttr).round(2)\n",
    "bittr = post_hoc_scheffe(a1_bilogarithmic_ttr, a2_bilogarithmic_ttr, b1_bilogarithmic_ttr, b2_bilogarithmic_ttr, c1_bilogarithmic_ttr).round(2)\n",
    "uber = post_hoc_scheffe(a1_uber_index, a2_uber_index, b1_uber_index, b2_uber_index, c1_uber_index).round(2)\n",
    "lwv = post_hoc_scheffe(a1_lexical_word_variation, a2_lexical_word_variation, b1_lexical_word_variation, b2_lexical_word_variation, c1_lexical_word_variation).round(2)\n",
    "vv1 = post_hoc_scheffe(a1_verb_variation_I, a2_verb_variation_I, b1_verb_variation_I, b2_verb_variation_I, c1_verb_variation_I).round(2)\n",
    "svv1 = post_hoc_scheffe(a1_squared_vv1, a2_squared_vv1, b1_squared_vv1, b2_squared_vv1, c1_squared_vv1).round(2)\n",
    "cvv1 = post_hoc_scheffe(a1_corrected_vv1, a2_corrected_vv1, b1_corrected_vv1, b2_corrected_vv1, c1_corrected_vv1).round(2)\n",
    "vv2 = post_hoc_scheffe(a1_verb_variation_II, a2_verb_variation_II, b1_verb_variation_II, b2_verb_variation_II, c1_verb_variation_II).round(2)\n",
    "nv = post_hoc_scheffe(a1_noun_variation, a2_noun_variation, b1_noun_variation, b2_noun_variation, c1_noun_variation).round(2)\n",
    "adj = post_hoc_scheffe(a1_adjective_variation, a2_adjective_variation, b1_adjective_variation, b2_adjective_variation, c1_adjective_variation).round(2)\n",
    "adv = post_hoc_scheffe(a1_adverb_variation, a2_adverb_variation, b1_adverb_variation, b2_adverb_variation, c1_adverb_variation).round(2)\n",
    "mv = post_hoc_scheffe(a1_modifier_variation, a2_modifier_variation, b1_modifier_variation, b2_modifier_variation, c1_modifier_variation).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6f845ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A1   A2   B1   B2   C1\n",
       "A1  1.0  0.0  0.0  0.0  0.0\n",
       "A2  0.0  1.0  0.0  0.0  0.0\n",
       "B1  0.0  0.0  1.0  1.0  0.0\n",
       "B2  0.0  0.0  1.0  1.0  0.0\n",
       "C1  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(ld)\n",
    "clean(ls1)\n",
    "clean(ls2)\n",
    "clean(vs1)\n",
    "clean(cvs1)\n",
    "clean(vs2)\n",
    "clean(ttr)\n",
    "clean(ndw)\n",
    "clean(cttr)\n",
    "clean(rttr)\n",
    "clean(bittr)\n",
    "clean(uber)\n",
    "clean(lwv)\n",
    "clean(vv1)\n",
    "clean(svv1)\n",
    "clean(cvv1)\n",
    "clean(vv2)\n",
    "clean(nv)\n",
    "clean(adj)\n",
    "clean(adv)\n",
    "clean(mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "85feeace",
   "metadata": {
    "id": "85feeace"
   },
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('post-hoc.xlsx') as writer:\n",
    "    # å°‡ DataFrame å¯«å…¥ Excel æ–‡ä»¶\n",
    "    ld.to_excel(writer, sheet_name='Lexical Density', index=False)\n",
    "    ls1.to_excel(writer, sheet_name='Lexical_Sophistication_I', index=False)\n",
    "    ls2.to_excel(writer, sheet_name='Lexical_Sophistication_II', index=False)\n",
    "    vs1.to_excel(writer, sheet_name='Verb_Sophistication_I', index=False)\n",
    "    cvs1.to_excel(writer, sheet_name='Corrected_Verb_Sophistication_I', index=False)\n",
    "    vs2.to_excel(writer, sheet_name='Verb_Sophistication_II', index=False)\n",
    "    ndw.to_excel(writer, sheet_name='Number_of_Different_Words', index=False)\n",
    "    ttr.to_excel(writer, sheet_name='Type-Token_Ratio', index=False)\n",
    "    cttr.to_excel(writer, sheet_name='Corrected_TTR', index=False)\n",
    "    rttr.to_excel(writer, sheet_name='Root_TTR', index=False)\n",
    "    bittr.to_excel(writer, sheet_name='Bilogarithmic_TTR', index=False)\n",
    "    uber.to_excel(writer, sheet_name='Uber_Index', index=False)\n",
    "    lwv.to_excel(writer, sheet_name='Lexical_Word_Variation', index=False)\n",
    "    vv1.to_excel(writer, sheet_name='Verb_Variation_I', index=False)\n",
    "    svv1.to_excel(writer, sheet_name='Squared_Verb_Variation_I', index=False)\n",
    "    cvv1.to_excel(writer, sheet_name='Corrected_Verb_Variation_I', index=False)\n",
    "    vv2.to_excel(writer, sheet_name='Verb_Variation_II', index=False)\n",
    "    nv.to_excel(writer, sheet_name='Noun_Variation', index=False)\n",
    "    adj.to_excel(writer, sheet_name='Adjective_Variation', index=False)\n",
    "    adv.to_excel(writer, sheet_name='Adverb_Variation', index=False)\n",
    "    mv.to_excel(writer, sheet_name='Modifier_Variation', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "93f59a7f",
   "metadata": {
    "id": "93f59a7f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "82a4e03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(index):\n",
    "    for column in index.columns:\n",
    "        for ind in index.index:\n",
    "            if index[column][ind]>0.00 and index[column][ind]<1.00:\n",
    "                if index[column][ind]<0.05:\n",
    "                    index[column][ind] = 1\n",
    "                else:\n",
    "                    index[column][ind] = 0\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f686a76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
